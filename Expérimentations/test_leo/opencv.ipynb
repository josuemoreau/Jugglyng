{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "social-victory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. CAP_PROP_BUFFERSIZE\n",
      "2. CAP_PROP_OPENNI_MAX_BUFFER_SIZE\n",
      "3. CAP_PROP_XI_ACQ_BUFFER_SIZE\n",
      "4. CAP_PROP_XI_ACQ_BUFFER_SIZE_UNIT\n",
      "5. CAP_PROP_XI_ACQ_TRANSPORT_BUFFER_SIZE\n",
      "6. CAP_PROP_XI_BUFFERS_QUEUE_SIZE\n",
      "7. CAP_PROP_XI_FFS_FILE_SIZE\n",
      "8. CAP_PROP_XI_FREE_FFS_SIZE\n",
      "9. CAP_PROP_XI_IMAGE_PAYLOAD_SIZE\n",
      "10. CAP_PROP_XI_USED_FFS_SIZE\n",
      "11. GFLUID_KERNEL_KIND_RESIZE\n",
      "12. GFluidKernel_Kind_Resize\n",
      "13. INTER_TAB_SIZE\n",
      "14. INTER_TAB_SIZE2\n",
      "15. LDR_SIZE\n",
      "16. WINDOW_AUTOSIZE\n",
      "17. WND_PROP_AUTOSIZE\n",
      "18. _INPUT_ARRAY_FIXED_SIZE\n",
      "19. _InputArray_FIXED_SIZE\n",
      "20. getOptimalDFTSize\n",
      "21. getTextSize\n",
      "22. resize\n",
      "23. resizeWindow\n",
      "24. ximgproc_segmentation_SelectiveSearchSegmentationStrategySize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(552, 1170, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import cv2 as cv\n",
    "import imutils\n",
    "imutils.find_function(\"size\")\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "authentic-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.119565217391304\n",
      "(643, 133, 741, 280)\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "cv.namedWindow('POULPY', cv.WINDOW_NORMAL)\n",
    "#cv.setWindowProperty('POULPY', cv.WND_PROP_ASPECT_RATIO, cv.WINDOW_KEEPRATIO)\n",
    "#cv.setWindowProperty('POULPY', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "img = cv.imread(\"poulpe_mignon.jpg\",cv.IMREAD_COLOR)\n",
    "cv.imshow(\"POULPY\", img)\n",
    "print(cv.getWindowProperty('POULPY', cv.WND_PROP_ASPECT_RATIO))\n",
    "a = cv.waitKey(0)\n",
    "height, width, _ = img.shape\n",
    "cv.resizeWindow('POULPY', width=width, height=height)\n",
    "a = cv.waitKey(0)\n",
    "print(cv.getWindowImageRect('POULPY'))\n",
    "cv.destroyAllWindows()\n",
    "cv.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "emerging-association",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n"
     ]
    }
   ],
   "source": [
    "#ATTENTION, mettre un chemin invalide ne renvoie pas d'erreur !\n",
    "import cv2 as cv\n",
    "import os\n",
    "import time\n",
    "path = \"../../../videos/noire_vers_croche.mp4\"\n",
    "if not os.path.exists(path):\n",
    "    raise IMportError\n",
    "cap = cv.VideoCapture(path)\n",
    "fps = round(cap.get(cv.CAP_PROP_FPS)) #cap.get(5)\n",
    "time_btw_frames = round(1000/fps)\n",
    "start = time.time()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        cap.set(cv.CAP_PROP_POS_FRAMES, 0)\n",
    "        ret, frame = cap.read()\n",
    "    print(cap.get(cv.CAP_PROP_POS_FRAMES))\n",
    "    cv.imshow(\"test\", frame)\n",
    "    if cv.waitKey(0) == 27: #27 correspond au code de la touche Echap.\n",
    "        break\n",
    "end = time.time()\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "christian-novel",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4deffeb56e28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;36m1000\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#Difficile de jouer la vidéo à la même vitesse que l'originale (à cause du délai de traitement ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Problème rsolvable. Régler au besoin le pb avec un while ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Besoin de ffmpeg/gstreamer pour fonctionner... ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "1000/cap.get(5)\n",
    "#Difficile de jouer la vidéo à la même vitesse que l'originale (à cause du délai de traitement ?)\n",
    "#Problème rsolvable. Régler au besoin le pb avec un while ?\n",
    "#Besoin de ffmpeg/gstreamer pour fonctionner... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "miniature-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTENTION, mettre un chemin invalide ne renvoie pas d'erreur !\n",
    "import cv2 as cv\n",
    "import os\n",
    "import time\n",
    "path = \"../../../videos/noire_vers_croche.mp4\"\n",
    "if not os.path.exists(path):\n",
    "    raise ImportError\n",
    "cap = cv.VideoCapture(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "prospective-legend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.get(cv.CAP_PROP_POS_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "transparent-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv.imshow(\"test\", frame)\n",
    "    if cv.waitKey(25) == 27: #27 correspond au code de la touche Echap.\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bound-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "drawing = False # true if mouse is pressed\n",
    "mode = True # if True, draw rectangle. Press 'm' to toggle to curve\n",
    "ix,iy = -1,-1\n",
    "# mouse callback function\n",
    "def draw_circle(event,x,y,flags,param):\n",
    "    global ix,iy,drawing,mode\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix,iy = x,y\n",
    "    elif event == cv.EVENT_MOUSEMOVE:\n",
    "        if drawing == True:\n",
    "            if mode == True:\n",
    "                cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n",
    "            else:\n",
    "                cv.circle(img,(x,y),5,(0,0,255),-1)\n",
    "    elif event == cv.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        if mode == True:\n",
    "            cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n",
    "        else:\n",
    "            cv.circle(img,(x,y),5,(0,0,255),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "concrete-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((512,512,3), np.uint8)\n",
    "cv.namedWindow('image')\n",
    "cv.setMouseCallback('image',draw_circle)\n",
    "while(1):\n",
    "    cv.imshow('image',img)\n",
    "    k = cv.waitKey(1) & 0xFF\n",
    "    if k == ord('m'):\n",
    "        mode = not mode\n",
    "    elif k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerical-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVENT_FLAG_ALTKEY', 'EVENT_FLAG_CTRLKEY', 'EVENT_FLAG_LBUTTON', 'EVENT_FLAG_MBUTTON', 'EVENT_FLAG_RBUTTON', 'EVENT_FLAG_SHIFTKEY', 'EVENT_LBUTTONDBLCLK', 'EVENT_LBUTTONDOWN', 'EVENT_LBUTTONUP', 'EVENT_MBUTTONDBLCLK', 'EVENT_MBUTTONDOWN', 'EVENT_MBUTTONUP', 'EVENT_MOUSEHWHEEL', 'EVENT_MOUSEMOVE', 'EVENT_MOUSEWHEEL', 'EVENT_RBUTTONDBLCLK', 'EVENT_RBUTTONDOWN', 'EVENT_RBUTTONUP']\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "events = [i for i in dir(cv) if 'EVENT' in i]\n",
    "print( events )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaptive-soccer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-26327d6df73f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcircles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcircles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    cimg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    circles = cv2.HoughCircles(cimg,cv2.HOUGH_GRADIENT,1,20,\n",
    "                               param1=50,param2=22,minRadius=15,maxRadius=18)\n",
    "\n",
    "    \n",
    "    l = len(circles[0,:])\n",
    "    for i in circles[0,:]:\n",
    "        cv2.circle(img,(i[0],i[1]),15,(0,0,255),5)\n",
    "    \n",
    "\n",
    "    cv2.imshow('detected circles',img)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "little-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fixed-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.legacy.MultiTracker_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "through-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function get:\n",
      "\n",
      "get(key, default=None, /) method of builtins.dict instance\n",
      "    Return the value for key if key is in the dictionary, else default.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = {1:True}\n",
    "d.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "postal-richardson",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'MultiTracker_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f26098223396>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# initialize OpenCV's special multi-object tracker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mtrackers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiTracker_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# if a video path was not supplied, grab the reference to the web cam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'MultiTracker_create'"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "'''ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-v\", \"--video\", type=str,\n",
    "    help=\"path to input video file\")\n",
    "ap.add_argument(\"-t\", \"--tracker\", type=str, default=\"kcf\",\n",
    "    help=\"OpenCV object tracker type\")\n",
    "args = {\"tracker\" : \"kcf\"}\n",
    "\n",
    "# initialize a dictionary that maps strings to their corresponding\n",
    "# OpenCV object tracker implementations\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "    \"csrt\": cv2.TrackerCSRT_create,\n",
    "    \"kcf\": cv2.TrackerKCF_create,\n",
    "    \"boosting\": cv2.TrackerBoosting_create,\n",
    "    \"mil\": cv2.TrackerMIL_create,\n",
    "    \"tld\": cv2.TrackerTLD_create,\n",
    "    \"medianflow\": cv2.TrackerMedianFlow_create,\n",
    "    \"mosse\": cv2.TrackerMOSSE_create\n",
    "}'''\n",
    "\n",
    "args = {}\n",
    "# initialize OpenCV's special multi-object tracker\n",
    "trackers = cv2.MultiTracker_create()\n",
    "\n",
    "# if a video path was not supplied, grab the reference to the web cam\n",
    "if not args.get(\"video\", False):\n",
    "    print(\"[INFO] starting video stream...\")\n",
    "    vs = VideoStream(src=0).start()\n",
    "    time.sleep(1.0)\n",
    "# otherwise, grab a reference to the video file\n",
    "else:\n",
    "    vs = cv2.VideoCapture(args[\"video\"])\n",
    "    \n",
    "# loop over frames from the video stream\n",
    "while True:\n",
    "    # grab the current frame, then handle if we are using a\n",
    "    # VideoStream or VideoCapture object\n",
    "    frame = vs.read()\n",
    "    frame = frame[1] if args.get(\"video\", False) else frame\n",
    "    # check to see if we have reached the end of the stream\n",
    "    if frame is None:\n",
    "        break\n",
    "    # resize the frame (so we can process it faster)\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    # grab the updated bounding box coordinates (if any) for each\n",
    "    # object that is being tracked\n",
    "    (success, boxes) = trackers.update(frame)\n",
    "    # loop over the bounding boxes and draw then on the frame\n",
    "    for box in boxes:\n",
    "        (x, y, w, h) = [int(v) for v in box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the 's' key is selected, we are going to \"select\" a bounding\n",
    "    # box to track\n",
    "    if key == ord(\"s\"):\n",
    "        # select the bounding box of the object we want to track (make\n",
    "        # sure you press ENTER or SPACE after selecting the ROI)\n",
    "        box = cv2.selectROI(\"Frame\", frame, fromCenter=False,\n",
    "            showCrosshair=True)\n",
    "        # create a new object tracker for the bounding box and add it\n",
    "        # to our multi-object tracker\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "        trackers.add(tracker, frame, box)\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    elif key == ord(\"q\"):\n",
    "        break\n",
    "# if we are using a webcam, release the pointer\n",
    "if not args.get(\"video\", False):\n",
    "    vs.stop()\n",
    "# otherwise, release the file pointer\n",
    "else:\n",
    "    vs.release()\n",
    "# close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "standing-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.lArgumentParser()\n",
    "ap.add_argument(\"-v\", \"--video\", type=str,\n",
    "    help=\"path to input video file\")\n",
    "ap.add_argument(\"-t\", \"--tracker\", type=str, default=\"kcf\",\n",
    "    help=\"OpenCV object tracker type\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize a dictionary that maps strings to their corresponding\n",
    "# OpenCV object tracker implementations\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "    \"csrt\": cv2.TrackerCSRT_create,\n",
    "    \"kcf\": cv2.TrackerKCF_create,\n",
    "    \"boosting\": cv2.TrackerBoosting_create,\n",
    "    \"mil\": cv2.TrackerMIL_create,\n",
    "    \"tld\": cv2.TrackerTLD_create,\n",
    "    \"medianflow\": cv2.TrackerMedianFlow_create,\n",
    "    \"mosse\": cv2.TrackerMOSSE_create\n",
    "}\n",
    "# initialize OpenCV's special multi-object tracker\n",
    "trackers = cv2.MultiTracker_create()\n",
    "\n",
    "# if a video path was not supplied, grab the reference to the web cam\n",
    "if not args.get(\"video\", False):\n",
    "    print(\"[INFO] starting video stream...\")\n",
    "    vs = VideoStream(src=0).start()\n",
    "    time.sleep(1.0)\n",
    "# otherwise, grab a reference to the video file\n",
    "else:\n",
    "    vs = cv2.VideoCapture(args[\"video\"])\n",
    "    \n",
    "# loop over frames from the video stream\n",
    "while True:\n",
    "    # grab the current frame, then handle if we are using a\n",
    "    # VideoStream or VideoCapture object\n",
    "    frame = vs.read()\n",
    "    frame = frame[1] if args.get(\"video\", False) else frame\n",
    "    # check to see if we have reached the end of the stream\n",
    "    if frame is None:\n",
    "        break\n",
    "    # resize the frame (so we can process it faster)\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    # grab the updated bounding box coordinates (if any) for each\n",
    "    # object that is being tracked\n",
    "    (success, boxes) = trackers.update(frame)\n",
    "    # loop over the bounding boxes and draw then on the frame\n",
    "    for box in boxes:\n",
    "        (x, y, w, h) = [int(v) for v in box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the 's' key is selected, we are going to \"select\" a bounding\n",
    "    # box to track\n",
    "    if key == ord(\"s\"):\n",
    "        # select the bounding box of the object we want to track (make\n",
    "        # sure you press ENTER or SPACE after selecting the ROI)\n",
    "        box = cv2.selectROI(\"Frame\", frame, fromCenter=False,\n",
    "            showCrosshair=True)\n",
    "        # create a new object tracker for the bounding box and add it\n",
    "        # to our multi-object tracker\n",
    "        tracker = OPENCV_OBJECT_TRACKERS[args[\"tracker\"]]()\n",
    "        trackers.add(tracker, frame, box)\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    elif key == ord(\"q\"):\n",
    "        break\n",
    "# if we are using a webcam, release the pointer\n",
    "if not args.get(\"video\", False):\n",
    "    vs.stop()\n",
    "# otherwise, release the file pointer\n",
    "else:\n",
    "    vs.release()\n",
    "# close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-arctic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respected-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Direction Warning:  8.06225774829855\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Direction Warning:  7.615773105863909\n",
      "Rebuilding history...\n",
      "Direction Warning:  10.04987562112089\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "Rebuilding history...\n",
      "finished writing data\n"
     ]
    }
   ],
   "source": [
    "import cv2, math, numpy as np\n",
    "# --------------------- START PARAMETERS -----------------------\n",
    "# h,s,v range of the object to be tracked - get these values with hsv_color_picker.py\n",
    "color_values = 128,119,55,157,255,126\n",
    "#color_values = 115,110,45,175,255,186\n",
    "color_values = 126,107,7,206,255,152\n",
    "\n",
    "# Capture your source video\n",
    "cap = cv2.VideoCapture(\"../../videos/noire_vers_croche.mp4\")\n",
    "# Pick a path to save the data\n",
    "output_path = \"../../videos/noire_vers_crocheM.mp4\"\n",
    "# How big is a ball? This is the dark blue circle (9 to 60)\n",
    "ball_size = 15\n",
    "# How big of a colored swatch is necessary to deam it as a ball (25 to 81 is good)\n",
    "contour_ball_size = 15\n",
    "global max_snap_distance, max_direction_deviation\n",
    "# The tracker 'snaps' onto the ball's position. 45 is good - light blue circle\n",
    "max_snap_distance = 20\n",
    "# The ball shouldn't change direction drastically 9 is good for 120fps\n",
    "max_direction_deviation = 35\n",
    "deviation_warning = 5\n",
    "# If the tracker gets lost, the last few tracking values are erroneous\n",
    "# Preserving the history allows the user can skip back a bit when a tracker error occurs\n",
    "p_imgs, history_length, history_idx = [], 30, 0\n",
    "# -------------------- END PARAMETERS ----------------------\n",
    "# Parameters for the text in the user instructions\n",
    "font, scale, color, thick = cv2.FONT_HERSHEY_SIMPLEX, .5, (255,0,0), 1\n",
    "# Takes an image, and a lower and upper bound\n",
    "# Returns only the parts of the image in bounds\n",
    "def only_color(frame, color_valeus, p_position):\n",
    "    # Create a mask\n",
    "    mask = np.zeros((frame.shape[0], frame.shape[1]), np.uint8)\n",
    "    # Draw a circle around the last known position of the ball\n",
    "    # Everything outside the circle is value=0\n",
    "    # Only objects inside the circle are possible juggling balls\n",
    "    mask = cv2.circle(mask, p_position, max_snap_distance, 255, -1)\n",
    "    # Mask out the image\n",
    "    frame = cv2.bitwise_and(frame, frame, mask=mask)    \n",
    "    # Convert BGR to HSV\n",
    "    b,r,g,b1,r1,g1 = color_values\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # Define range of blue color in HSV\n",
    "    lower, upper = np.array([b,r,g]), np.array([b1,r1,g1])\n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    #cv2.imshow('mask', mask)\n",
    "    return res, mask\n",
    "# Finds the largest contour in a list of contours\n",
    "# Returns a single contour\n",
    "def largest_contour(contours):\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    return c[0]\n",
    "\n",
    "# Takes an image and the threshold value returns the contours\n",
    "def get_contours(im):\n",
    "    imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    _ ,thresh = cv2.threshold(imgray,0,255,0)\n",
    "    contours, _ = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours\n",
    "\n",
    "# Finds the center of a contour\n",
    "# Takes a single contour\n",
    "# Returns (x,y) position of the contour\n",
    "def contour_center(c):\n",
    "    M = cv2.moments(c)\n",
    "    try: center = int(M['m10']/M['m00']), int(M['m01']/M['m00'])\n",
    "    except: center = 0,0\n",
    "    return center\n",
    "\n",
    "# Finds the most likely position of the ball in an image using color\n",
    "# Returns xy coordinates\n",
    "def find_ball_by_color(img, color_values, ball_position):\n",
    "    # Filter the image for the specific ball color\n",
    "    img, mask = only_color(img, color_values, ball_position)    \n",
    "    # Find the contours in the image\n",
    "    contours = get_contours(img)\n",
    "    # Start with values that are definitely out of the snap range\n",
    "    nearest_ball = 100000000\n",
    "    nearest_snap_possibility = (10000,10000)\n",
    "    # Iterate through the contours\n",
    "    for contour in contours:\n",
    "        # Check if a contour is big enough to be considered a ball\n",
    "        if cv2.contourArea(contour)>contour_ball_size:\n",
    "            # Get the center of the contour\n",
    "            center = contour_center(contour)\n",
    "            # This contour is the nearest one to the ball, it is the most likely snap position\n",
    "            if distance(center, ball_position) < nearest_ball:\n",
    "                # Save this as the nearest likely ball\n",
    "                nearest_snap_possibility = center\n",
    "    # Return the mostly likely location of the ball in the image\n",
    "    return nearest_snap_possibility\n",
    "\n",
    "def distance(a,b): return math.sqrt((a[0]-b[0])**2+(a[1]-b[1])**2)\n",
    "\n",
    "# Parameters of lk optical flow\n",
    "lk_params = dict(winSize = (ball_size, ball_size),\n",
    "                 maxLevel = 5,\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS |\n",
    "                             cv2.TERM_CRITERIA_COUNT, 5, 0.03))\n",
    "\n",
    "# Mouse callback function\n",
    "global click_list\n",
    "positions, click_list = [], []\n",
    "def callback(event, x, y, flags, param):\n",
    "    if event == 1: click_list.append((x,y))\n",
    "cv2.namedWindow('img')\n",
    "cv2.setMouseCallback('img', callback)\n",
    "\n",
    "# Initialize frame_number\n",
    "frame_number = 0\n",
    "wait_time = 0\n",
    "\n",
    "# Initialize tracker value, this will get reset by the user\n",
    "p0 = np.array([[[0,0]]], np.float32)\n",
    "fails = 0\n",
    "# Main loop\n",
    "while True:\n",
    "\n",
    "    # Program crashes if user tries to use the history before it is built completely \n",
    "    if len(p_imgs)< history_length - history_idx: print(\"Rebuilding history...\")\n",
    "    \n",
    "    # Read frame of video (either from the source or the history)\n",
    "    if history_idx == 0:\n",
    "        ret, img = cap.read()\n",
    "        try:\n",
    "            #img = cv2.resize(img, (1908, 1080)) #resize the image so it fills screen\n",
    "            #img = cv2.blur(img, (15,15))\n",
    "            pass\n",
    "        except: break\n",
    "        # Save the frame to the history\n",
    "        if ret: p_imgs.append(img.copy())\n",
    "    else:\n",
    "        # Read a frame from the history\n",
    "        img = p_imgs.pop(0)\n",
    "        history_idx -= 1\n",
    "\n",
    "    # Break if the video is over\n",
    "    try:h,w,e = img.shape\n",
    "    except: break\n",
    "\n",
    "    # Optical flow works in single channel, so save the previous gray image\n",
    "    try: old_gray = img_gray.copy()\n",
    "    except: old_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    try: img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    except: break\n",
    "\n",
    "    # Track using optical flow\n",
    "    # p1 is the value produced by the optical flow tracker\n",
    "    p1, _, _ = cv2.calcOpticalFlowPyrLK(old_gray, img_gray,p0, None, **lk_params)\n",
    "\n",
    "    # Convert the coordinates to integers for drawing\n",
    "    xy  = int(p1[0][0][0]), int(p1[0][0][1])\n",
    "\n",
    "    # Get the nearest likely snap position\n",
    "    snap_position = find_ball_by_color(img, color_values, xy)\n",
    "    # Draw a circle around the snap position showing the max snap distance and max directional deviation\n",
    "    cv2.circle(img, snap_position, max_snap_distance, (255,255,0), 1)\n",
    "    cv2.circle(img, snap_position, max_direction_deviation, (0,255,255), 1)\n",
    "    # Is the likely position found using color close to the optical flow tracker\n",
    "    if distance(snap_position, xy) < max_snap_distance:\n",
    "        # The color value is a good one, so snap to that\n",
    "        xy = snap_position\n",
    "        p1 = np.array([[[snap_position[0], snap_position[1]]]], np.float32)\n",
    "\n",
    "    # Update the predicted tracking point\n",
    "    p0 = p1\n",
    "\n",
    "    # If there are at least two positions, use those positions to calculate a likely position\n",
    "    # where the ball will be in the future, if the ball isn't there, tracker failure!\n",
    "    if len(positions)>2:\n",
    "        # Get the last two tracking locations\n",
    "        past, present = positions[-2:]\n",
    "        # Calculate the predicted future position of the ball\n",
    "        dx, dy = present[0]-past[0], present[1]-past[1]\n",
    "        future = present[0]+dx, present[1]+dy\n",
    "        #print(dx,dy, distance(future, xy))\n",
    "        cv2.circle(img, future, 2, (255, 255, 255), -1)\n",
    "        # If the tracking location and the predicted location don't match, stop the tracker and tell the user\n",
    "        if distance(future, xy) > deviation_warning:\n",
    "            print(\"Direction Warning: \" , distance(future, xy))\n",
    "        else:\n",
    "            if fails > 0: fails -=1\n",
    "        if distance(future, xy) > max_direction_deviation:\n",
    "            print(\"Direction Fail: \", distance(future, xy))\n",
    "            wait_time = 0           \n",
    "    \n",
    "    # Make circle around ball to show the tracking point\n",
    "    cv2.circle(img, xy, ball_size, (255,2,1), 1)\n",
    "\n",
    "    # Write instructions to user\n",
    "    if wait_time == 0:\n",
    "        cv2.putText(img, \"Click on ball and press F5 to reset tracker\", (50,50), font, scale, color, thick, cv2.LINE_AA)\n",
    "        if len(positions)>0:\n",
    "            cv2.putText(img, \"...or press F6 to continue to the next frame\", (50,80), font, scale, color, thick, cv2.LINE_AA)\n",
    "            cv2.putText(img, \"...or press F7 to autotrack slowly\", (50,110), font, scale, color, thick, cv2.LINE_AA)\n",
    "            cv2.putText(img, \"...or press F8 to autotrack at max speed\", (50,140), font, scale, color, thick, cv2.LINE_AA)\n",
    "            cv2.putText(img, \"...or press F4 to use the predicted value (white dot)\", (50,170), font, scale, color, thick, cv2.LINE_AA)\n",
    "    if wait_time == 1 or wait_time == 25:\n",
    "        cv2.putText(img, \"Auto-Tracking\", (50,50), font, scale, color, thick, cv2.LINE_AA)\n",
    "        cv2.putText(img, \"Press any key on mistake\", (50,80), font, scale, color, thick, cv2.LINE_AA)\n",
    "        cv2.putText(img, \"Press F6 to pause\", (50,110), font, scale, color, thick, cv2.LINE_AA)\n",
    "\n",
    "    # Draw the previous ball positions on the screen\n",
    "    idx = 0\n",
    "    while idx < 20 and idx < len(positions)-3:\n",
    "        a,b = positions[-(idx+2)], positions[-(idx+1)]\n",
    "        cv2.line(img, a,b, (0,255,86), 4)\n",
    "        idx += 1\n",
    "\n",
    "    # Show frame and wait\n",
    "    cv2.imshow('img', img)\n",
    "    k = cv2.waitKey(wait_time)\n",
    "    if k ==27: break\n",
    "\n",
    "    # User presses F4, use the the predicted value\n",
    "    if k == 193:\n",
    "        xy = future\n",
    "        p0 = np.array([[xy]], np.float32)\n",
    "    # User Presses F6, advance to the next frame and wait\n",
    "    if k == 195: wait_time = 0\n",
    "    # User presses F7, play slowly\n",
    "    if k == 196: wait_time = 65\n",
    "    # User presses F8, max computer resources\n",
    "    if k == 197: wait_time = 1\n",
    "    # User wants to reset the tracker to the last click position \n",
    "    if k == 194: #F5\n",
    "        # Get the user's input\n",
    "        xy = click_list[-1]\n",
    "        # Set the tracker to this user input\n",
    "        p0 = np.array([[xy]], np.float32)\n",
    "        # Advance to the next frame, but pause\n",
    "        wait_time = 0    \n",
    "\n",
    "    # Add tracked location to data\n",
    "    positions.append(xy)\n",
    "\n",
    "    # If there are enough items in the history, remove the oldest one\n",
    "    if len(p_imgs)>history_length: p_imgs.pop(0)\n",
    "\n",
    "    # User has paused any other key because the tracker has gotten lost\n",
    "    if k < 190 and k >10:\n",
    "        # Pause\n",
    "        wait_time = 0\n",
    "        # Get the history length\n",
    "        history_idx = history_length\n",
    "        # Remove the last few positions, that are errors from a broken tracker\n",
    "        positions = positions[:-history_length]\n",
    "\n",
    "# Close the window and release the source video\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "\n",
    "\n",
    "# Write data to a csv (spreadsheet) file\n",
    "import csv\n",
    "with open(output_path, 'w') as csvfile:\n",
    "    fieldnames = ['x_position', 'y_position']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "    for position in positions:\n",
    "        x, y = position[0], position[1]\n",
    "        writer.writerow({'x_position': x, 'y_position': y})\n",
    "print( 'finished writing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-heading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-stewart",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
